{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 557/557 [00:00<00:00, 274kB/s]\n",
      "Downloading: 100%|██████████| 518M/518M [03:10<00:00, 2.84MB/s] \n",
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "phobert = AutoModel.from_pretrained(\"vinai/phobert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 874k/874k [00:01<00:00, 506kB/s]  \n",
      "Downloading: 100%|██████████| 1.08M/1.08M [00:01<00:00, 800kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# For transformers v4.x+: \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT TEXT MUST BE ALREADY WORD-SEGMENTED!\n",
    "sentence = 'Chúng_tôi là những nghiên_cứu_viên .' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([tokenizer.encode(sentence)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,   746,     8,    21, 46349,     5,     2]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    features = phobert(input_ids)  # Models outputs are now tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.2803,  0.1214, -0.1942,  ...,  0.4171, -0.0161, -0.1955],\n",
       "         [ 0.1492,  0.1822, -0.2678,  ..., -0.0205, -0.6449,  0.2517],\n",
       "         [ 0.2523, -0.2934,  0.0205,  ...,  0.2748,  0.0502,  0.5852],\n",
       "         ...,\n",
       "         [ 0.0836,  0.1797,  0.2654,  ...,  0.5300,  0.1707,  0.0412],\n",
       "         [-0.3348, -0.1969, -0.0231,  ...,  0.2369,  0.3467,  0.1302],\n",
       "         [ 0.1441,  0.0710, -0.1136,  ...,  0.5399, -0.1935, -0.2496]]]), pooler_output=tensor([[ 3.2356e-01,  6.8807e-02,  2.4668e-01, -1.2170e-02, -5.0038e-02,\n",
       "         -5.9722e-02,  1.3742e-01, -9.0525e-02, -4.1215e-02, -2.0762e-01,\n",
       "          7.6809e-03, -1.7198e-02,  6.5538e-02, -2.5962e-01,  2.2038e-01,\n",
       "         -1.3961e-01,  1.3031e-01,  7.7056e-02, -5.0066e-01,  5.6116e-04,\n",
       "          6.7693e-02, -4.0129e-01,  8.4473e-02, -1.1812e-01,  3.5882e-02,\n",
       "         -1.9702e-01,  4.3108e-03,  1.2938e-01,  1.4372e-01,  1.9209e-01,\n",
       "         -1.6580e-03, -2.1453e-01,  1.8837e-01,  3.6349e-01, -2.8961e-01,\n",
       "         -1.9030e-01, -5.0979e-01, -7.9987e-02, -7.8155e-02,  7.9036e-02,\n",
       "          3.2819e-01,  2.2303e-01, -1.4707e-01, -2.0958e-01, -1.3165e-02,\n",
       "          3.6204e-02,  4.0968e-01,  4.0113e-01, -1.2507e-01, -1.0427e-01,\n",
       "          1.5594e-01,  2.4970e-02, -7.9546e-03,  1.9481e-01,  3.7922e-01,\n",
       "         -1.8919e-01, -9.0387e-02,  5.7534e-01,  1.9403e-02,  2.4631e-01,\n",
       "          3.6759e-01,  1.9244e-01, -4.5748e-01,  8.3363e-02,  1.6080e-01,\n",
       "          1.3813e-03,  4.1321e-01,  8.3819e-02, -2.5578e-02,  2.6782e-02,\n",
       "          2.7782e-01, -1.7845e-01,  1.4933e-01, -1.4904e-01,  1.5547e-01,\n",
       "         -2.9961e-01, -9.7159e-02,  1.1874e-02, -1.9497e-02,  2.4088e-01,\n",
       "          2.4776e-01,  1.9448e-02, -4.0194e-02,  4.5451e-01, -1.6213e-01,\n",
       "         -9.0387e-03,  3.2908e-01, -1.5880e-01,  3.9583e-02,  2.5313e-01,\n",
       "          3.5398e-02, -4.8262e-02, -3.0094e-02, -1.1079e-01, -3.9099e-02,\n",
       "         -4.5695e-01,  8.4270e-02, -4.4468e-02, -6.9188e-02,  1.8748e-01,\n",
       "         -6.7908e-02,  3.3535e-03,  7.8092e-03, -4.8371e-01, -1.2673e-01,\n",
       "         -2.5799e-02, -2.8293e-01, -7.6424e-02,  5.0793e-01, -2.2678e-01,\n",
       "         -2.1026e-01, -9.4730e-02,  3.1274e-01,  1.4297e-01,  2.1836e-02,\n",
       "          8.5986e-02,  2.7609e-01,  2.7106e-02, -3.9507e-01,  2.7772e-01,\n",
       "          1.6772e-01,  2.0392e-01,  2.4315e-02,  2.4308e-01, -4.3086e-01,\n",
       "          3.1683e-01, -3.0984e-01, -1.1825e-01, -9.7634e-02,  8.0094e-02,\n",
       "         -1.4221e-01,  2.9731e-01, -7.9446e-02, -4.8730e-01,  1.5322e-02,\n",
       "         -2.1076e-01,  4.5793e-02, -3.6050e-02, -4.7809e-02, -2.4399e-01,\n",
       "         -1.6253e-02,  8.2101e-02,  7.8659e-03, -2.6124e-01,  4.1198e-02,\n",
       "          5.1005e-02, -2.9525e-03, -3.3699e-01,  2.6410e-01,  1.0203e-01,\n",
       "         -4.3313e-01,  4.1545e-01, -1.8098e-02,  3.9197e-01, -3.4087e-01,\n",
       "         -1.3986e-01,  1.6644e-01,  1.5676e-01, -2.7380e-01,  4.8115e-02,\n",
       "         -3.1412e-01, -3.3218e-02, -2.6742e-02, -3.6407e-01, -6.9017e-02,\n",
       "          1.5290e-01,  1.4284e-01,  6.3094e-02, -1.7220e-01, -7.9393e-03,\n",
       "         -2.3329e-01, -2.3385e-01, -1.6247e-01,  2.2047e-01,  1.3658e-01,\n",
       "         -1.7160e-01,  3.4056e-01,  1.8547e-01, -3.6659e-01,  1.2483e-01,\n",
       "          2.3268e-01, -3.6866e-01,  2.0118e-01,  1.0062e-01, -1.1363e-01,\n",
       "         -3.5851e-01, -1.4581e-01, -2.1665e-01,  2.8467e-01,  2.1944e-01,\n",
       "         -8.6968e-02,  3.3603e-01, -1.3291e-01,  7.6470e-02, -3.9384e-01,\n",
       "          8.7070e-02,  1.8656e-01,  3.3074e-01,  8.4197e-02, -2.3073e-01,\n",
       "         -5.0470e-02,  3.0444e-01,  3.2446e-01,  1.9457e-03, -1.4427e-01,\n",
       "         -6.4884e-02, -5.6954e-02,  7.5328e-02,  6.0543e-03, -1.6515e-01,\n",
       "         -2.0974e-01,  1.7649e-01,  2.5276e-01,  2.7464e-02, -3.4663e-02,\n",
       "          1.1239e-01,  3.5827e-01, -2.8111e-01,  3.5358e-02,  4.9653e-02,\n",
       "         -1.1553e-01,  1.3479e-02,  1.1016e-01,  2.6401e-01, -3.4379e-02,\n",
       "          2.1285e-01,  2.1025e-01, -1.6921e-01, -6.7291e-02, -9.5234e-02,\n",
       "         -3.6467e-01,  1.2736e-01, -4.3015e-01,  3.8770e-02,  6.3262e-02,\n",
       "          4.0906e-03,  1.2522e-01, -4.5083e-02, -6.1746e-03,  1.0449e-01,\n",
       "         -2.8166e-01, -6.7917e-02,  1.9363e-01, -2.5945e-01,  9.3389e-02,\n",
       "         -1.4383e-01, -3.6516e-01,  9.8813e-02, -9.4105e-02,  4.1844e-02,\n",
       "          2.4313e-01, -5.8586e-02, -5.6884e-02, -2.4146e-01, -2.7238e-02,\n",
       "         -2.9969e-01, -3.0211e-01,  3.1738e-01,  8.1288e-02, -5.4190e-02,\n",
       "         -1.2376e-01, -4.9828e-02, -4.8127e-02, -1.5888e-01, -2.4704e-01,\n",
       "         -1.2685e-01, -5.7327e-02,  5.0149e-02, -1.3170e-01, -7.1959e-02,\n",
       "          6.1776e-02, -1.8514e-01,  2.4813e-02, -1.1305e-01,  8.9890e-02,\n",
       "          8.5525e-02, -2.1873e-01, -1.8055e-01, -2.9958e-01, -1.7488e-01,\n",
       "          2.5030e-01, -4.1080e-02, -2.3178e-02,  1.0520e-01,  2.1074e-01,\n",
       "          3.1270e-01, -1.4219e-01, -2.8470e-02, -7.1510e-02,  7.3036e-02,\n",
       "         -1.8335e-01,  4.7342e-02, -6.3725e-02, -5.0386e-02,  1.8333e-01,\n",
       "          1.4460e-01,  1.8819e-01, -3.4915e-03,  1.8639e-01,  4.0673e-01,\n",
       "         -3.3623e-01, -7.8663e-04, -1.2423e-01, -2.4671e-01,  1.2208e-01,\n",
       "         -3.9579e-01, -3.6336e-02, -8.8275e-02,  6.9799e-02,  1.1515e-01,\n",
       "          3.9912e-02,  1.5816e-01,  4.7877e-01, -2.9517e-01, -1.9245e-01,\n",
       "          2.3582e-01,  5.0040e-01,  1.2628e-01, -3.6173e-01, -1.0846e-02,\n",
       "          2.1432e-01,  1.2097e-01, -1.4738e-01,  1.8796e-01,  5.3722e-02,\n",
       "          2.7100e-01,  1.3108e-01, -3.0986e-03, -9.2006e-02,  3.4008e-01,\n",
       "          9.6987e-02,  2.9751e-02,  7.8753e-02,  2.5953e-01,  1.8564e-01,\n",
       "         -3.4745e-01, -1.1513e-01, -2.7325e-01,  1.8787e-01, -5.9595e-02,\n",
       "          4.9905e-02, -4.6738e-02, -7.1610e-02, -2.3592e-01,  5.0210e-02,\n",
       "          4.2938e-01, -1.0434e-01,  5.5898e-02, -6.1959e-02, -7.7933e-02,\n",
       "          1.0612e-01, -2.9068e-01, -1.6760e-01,  1.5046e-01, -3.1198e-02,\n",
       "         -1.4458e-01, -1.9105e-01,  1.3221e-01, -1.3021e-01, -5.3511e-02,\n",
       "          3.0757e-01,  2.7805e-01,  1.1379e-01,  3.0156e-02, -6.3840e-03,\n",
       "          3.7170e-01,  2.0467e-01, -5.0029e-02, -2.5668e-01, -4.8801e-02,\n",
       "          3.4724e-02, -1.9134e-01,  3.0109e-01, -4.4690e-02,  2.1682e-02,\n",
       "          2.1775e-01,  1.4079e-02,  7.7059e-03,  1.6973e-01,  2.0835e-01,\n",
       "          1.4216e-01, -5.7731e-02, -1.1384e-01, -3.6062e-01, -1.3935e-01,\n",
       "         -2.2808e-01, -1.0972e-01,  2.0820e-02,  1.4127e-01,  3.4593e-01,\n",
       "         -9.4599e-02,  3.2318e-01, -3.4793e-01, -2.2841e-01, -1.3870e-01,\n",
       "         -2.2551e-01,  2.0208e-01,  2.1096e-01,  1.9063e-01,  2.3811e-01,\n",
       "          1.0764e-01, -3.6371e-02, -2.6057e-01,  2.2809e-02, -1.5918e-02,\n",
       "          1.6952e-01,  1.4532e-01, -1.1566e-02,  2.9290e-02,  1.3691e-01,\n",
       "          3.6294e-03, -3.1097e-01,  3.8894e-02,  8.9511e-02, -2.4559e-01,\n",
       "          3.4412e-01,  2.8584e-01, -7.4922e-02, -9.6098e-02, -2.7859e-01,\n",
       "         -1.1786e-01, -1.3959e-02, -1.9478e-01, -6.0740e-02, -2.5595e-01,\n",
       "          3.5289e-02,  1.0516e-01,  1.1509e-02,  4.2218e-01,  5.3241e-01,\n",
       "          8.2289e-02, -2.1476e-01, -1.8364e-01, -2.8287e-01, -4.7442e-02,\n",
       "          3.6418e-02, -1.0004e-01,  1.4397e-01,  5.4881e-01,  1.1615e-01,\n",
       "          1.0881e-01, -1.3113e-02,  4.4165e-01, -2.1154e-01,  1.9887e-01,\n",
       "         -2.3201e-01, -3.8439e-02,  1.3247e-01,  4.5898e-02,  3.5681e-03,\n",
       "          2.8840e-01, -3.4101e-01, -1.3250e-01,  3.1379e-02,  1.8129e-01,\n",
       "          1.6333e-01, -2.3487e-02,  5.1262e-01,  1.2675e-01, -5.0250e-02,\n",
       "          3.2816e-02, -3.7918e-02,  8.9821e-02, -8.8036e-02,  3.6810e-01,\n",
       "         -6.3277e-02, -3.9600e-01,  2.9552e-01, -7.4217e-02, -3.2124e-01,\n",
       "          1.1370e-03,  2.9951e-01,  1.2760e-03,  6.6064e-02,  2.6129e-01,\n",
       "          2.1038e-01,  5.6630e-02, -9.0695e-02, -3.4116e-01, -1.3164e-01,\n",
       "          1.3704e-02,  3.8093e-02,  4.1691e-01, -1.5666e-01,  2.7354e-01,\n",
       "          8.7664e-03,  4.8012e-02,  3.7859e-01,  7.8286e-02, -8.0737e-02,\n",
       "         -2.6279e-01,  2.2989e-01,  5.9378e-02, -2.0865e-01,  1.8409e-01,\n",
       "         -2.4377e-02,  2.2591e-01,  5.4951e-02,  3.4647e-01, -6.4766e-02,\n",
       "          1.5379e-02,  4.2052e-01, -1.0564e-01,  1.7925e-01,  1.6929e-01,\n",
       "          1.1215e-01, -1.7638e-01,  2.6777e-02,  3.3063e-01,  1.5712e-02,\n",
       "         -1.2517e-01,  1.7321e-01,  3.9652e-01,  2.3057e-01,  1.1005e-01,\n",
       "         -2.1206e-01,  1.3877e-01,  3.1534e-01,  9.1146e-02, -2.8163e-01,\n",
       "         -5.9605e-02,  1.9173e-01,  7.0849e-02,  3.9846e-02,  4.6931e-02,\n",
       "         -3.5598e-01,  1.3598e-01,  8.9619e-02, -9.8157e-02, -1.0389e-01,\n",
       "         -2.3553e-01,  8.7716e-02, -1.0404e-01, -3.9487e-01, -2.1300e-01,\n",
       "          2.5726e-02,  1.6104e-02, -5.8521e-02, -3.6555e-01, -1.7610e-01,\n",
       "         -2.5911e-01, -9.3786e-03,  4.1170e-02,  1.5552e-02,  2.3371e-01,\n",
       "          3.8034e-01,  1.1459e-01,  2.2010e-01, -2.9182e-02, -2.1185e-01,\n",
       "         -1.0078e-01,  1.6090e-01, -2.5305e-01,  9.0957e-02, -1.0630e-01,\n",
       "          1.3166e-01,  2.8412e-02, -1.1403e-01, -2.3070e-01,  1.9142e-01,\n",
       "          7.2827e-02,  1.0077e-01, -8.5275e-02, -7.6405e-02,  2.8028e-02,\n",
       "          1.0881e-01,  3.6664e-01, -2.1130e-01,  4.2561e-02, -1.2023e-01,\n",
       "         -1.0817e-01,  1.5248e-02,  1.4776e-01,  1.7676e-01,  1.5140e-01,\n",
       "          2.6131e-01, -1.0666e-01,  1.0539e-02,  2.1011e-01,  9.4795e-02,\n",
       "         -3.2175e-02,  1.2351e-01, -1.8147e-02,  2.9107e-02, -4.8487e-02,\n",
       "          2.2995e-01, -3.6445e-02, -2.5212e-01,  2.5256e-01, -7.6358e-03,\n",
       "          3.2285e-01, -2.7538e-01,  6.1627e-03, -1.9838e-01,  1.0047e-01,\n",
       "          9.3331e-02, -1.7859e-01, -1.2384e-01,  3.1713e-01, -2.1902e-01,\n",
       "         -1.1613e-01,  3.1179e-01, -2.4689e-02, -3.3935e-01, -1.8425e-01,\n",
       "         -8.8343e-02,  1.7959e-01, -4.2800e-03,  9.3990e-02, -8.2420e-02,\n",
       "          3.5891e-01, -9.6190e-02, -2.3118e-01, -3.3623e-01,  9.6662e-02,\n",
       "         -1.2626e-01, -1.9176e-01,  2.6696e-01,  2.7641e-01, -1.5231e-01,\n",
       "          1.8127e-01,  3.2665e-02,  2.4520e-01,  2.3004e-01,  7.9890e-02,\n",
       "          3.4158e-02, -5.2496e-02, -5.0806e-03, -2.6299e-01, -1.0342e-01,\n",
       "         -2.0025e-01, -1.9415e-01, -6.6728e-02, -2.6080e-01, -5.4599e-01,\n",
       "          1.1557e-01,  2.1583e-01, -3.4713e-01, -1.5882e-01, -5.9560e-03,\n",
       "          3.8051e-01,  3.2214e-01, -3.5320e-01, -8.4185e-02, -1.1656e-01,\n",
       "         -1.0125e-01, -2.8393e-01,  3.2832e-01, -1.4416e-01,  1.0282e-01,\n",
       "         -5.7437e-02, -1.3223e-02, -2.4799e-02,  6.1816e-04,  7.0731e-02,\n",
       "         -1.0808e-01, -1.4009e-01,  1.7373e-01, -1.4521e-01, -7.8702e-02,\n",
       "         -6.0216e-02, -4.2313e-02, -2.9258e-01,  8.2496e-02, -1.2742e-01,\n",
       "          3.3914e-01,  7.5809e-02, -4.8059e-02, -2.9438e-01,  2.7892e-01,\n",
       "          1.4789e-01,  7.2331e-02, -7.9040e-02,  1.3006e-01,  2.0975e-01,\n",
       "          1.7561e-02, -3.4268e-01, -2.7370e-01, -1.9998e-01,  2.5381e-01,\n",
       "          1.8589e-01, -3.4034e-01,  2.0634e-01,  1.9083e-02,  2.3755e-01,\n",
       "          1.3653e-01,  2.3858e-01,  6.6519e-02, -7.3879e-02, -2.4172e-01,\n",
       "         -4.0096e-02, -7.3598e-02, -2.2265e-01, -8.8047e-02, -1.3823e-01,\n",
       "         -5.0333e-02, -2.6405e-01, -3.0174e-01, -4.2550e-01, -6.0004e-02,\n",
       "          3.2633e-01,  7.6712e-03, -3.5559e-01,  1.8749e-01, -6.5799e-02,\n",
       "         -1.2707e-02,  3.7685e-01, -2.6955e-01, -3.2532e-02, -2.5805e-01,\n",
       "         -3.1422e-02, -1.4983e-01,  2.7979e-01,  1.6932e-02, -2.4224e-01,\n",
       "          2.4903e-01, -4.3664e-02, -1.3503e-01,  6.7023e-02,  1.0661e-01,\n",
       "         -2.9826e-01, -4.2024e-01, -1.3812e-01, -1.9036e-01, -1.3940e-02,\n",
       "          5.1796e-01,  9.8783e-02,  3.1564e-01,  1.5257e-01,  2.2063e-02,\n",
       "          1.7211e-01,  2.7012e-01, -1.4930e-01,  1.6983e-01,  8.8252e-02,\n",
       "          9.5880e-02, -1.6698e-01, -7.2178e-02,  2.6378e-01,  9.5576e-02,\n",
       "          1.3019e-01,  1.0060e-02,  5.7025e-02,  1.6329e-01, -2.1116e-01,\n",
       "          1.4585e-01,  2.3094e-01,  4.8238e-01,  1.4406e-01, -1.5869e-01,\n",
       "         -1.0522e-01,  6.8848e-02,  8.0928e-03,  1.8317e-01,  2.9984e-01,\n",
       "         -1.2757e-01, -5.1485e-01, -2.9127e-01, -4.7411e-02,  1.0825e-01,\n",
       "         -4.5399e-02, -4.4384e-01,  3.5277e-01, -2.5786e-01,  2.8086e-01,\n",
       "         -5.8258e-02,  2.4465e-01, -6.2713e-02]]), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = features.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2803,  0.1214, -0.1942,  ...,  0.4171, -0.0161, -0.1955],\n",
       "         [ 0.1492,  0.1822, -0.2678,  ..., -0.0205, -0.6449,  0.2517],\n",
       "         [ 0.2523, -0.2934,  0.0205,  ...,  0.2748,  0.0502,  0.5852],\n",
       "         ...,\n",
       "         [ 0.0836,  0.1797,  0.2654,  ...,  0.5300,  0.1707,  0.0412],\n",
       "         [-0.3348, -0.1969, -0.0231,  ...,  0.2369,  0.3467,  0.1302],\n",
       "         [ 0.1441,  0.0710, -0.1136,  ...,  0.5399, -0.1935, -0.2496]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 768])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 746, 8, 21, 46349, 5, 2]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.2803,  0.1214, -0.1942,  ...,  0.4171, -0.0161, -0.1955],\n",
       "         [ 0.1492,  0.1822, -0.2678,  ..., -0.0205, -0.6449,  0.2517],\n",
       "         [ 0.2523, -0.2934,  0.0205,  ...,  0.2748,  0.0502,  0.5852],\n",
       "         ...,\n",
       "         [ 0.0836,  0.1797,  0.2654,  ...,  0.5300,  0.1707,  0.0412],\n",
       "         [-0.3348, -0.1969, -0.0231,  ...,  0.2369,  0.3467,  0.1302],\n",
       "         [ 0.1441,  0.0710, -0.1136,  ...,  0.5399, -0.1935, -0.2496]]]), pooler_output=tensor([[ 3.2356e-01,  6.8807e-02,  2.4668e-01, -1.2170e-02, -5.0038e-02,\n",
       "         -5.9722e-02,  1.3742e-01, -9.0525e-02, -4.1215e-02, -2.0762e-01,\n",
       "          7.6809e-03, -1.7198e-02,  6.5538e-02, -2.5962e-01,  2.2038e-01,\n",
       "         -1.3961e-01,  1.3031e-01,  7.7056e-02, -5.0066e-01,  5.6116e-04,\n",
       "          6.7693e-02, -4.0129e-01,  8.4473e-02, -1.1812e-01,  3.5882e-02,\n",
       "         -1.9702e-01,  4.3108e-03,  1.2938e-01,  1.4372e-01,  1.9209e-01,\n",
       "         -1.6580e-03, -2.1453e-01,  1.8837e-01,  3.6349e-01, -2.8961e-01,\n",
       "         -1.9030e-01, -5.0979e-01, -7.9987e-02, -7.8155e-02,  7.9036e-02,\n",
       "          3.2819e-01,  2.2303e-01, -1.4707e-01, -2.0958e-01, -1.3165e-02,\n",
       "          3.6204e-02,  4.0968e-01,  4.0113e-01, -1.2507e-01, -1.0427e-01,\n",
       "          1.5594e-01,  2.4970e-02, -7.9546e-03,  1.9481e-01,  3.7922e-01,\n",
       "         -1.8919e-01, -9.0387e-02,  5.7534e-01,  1.9403e-02,  2.4631e-01,\n",
       "          3.6759e-01,  1.9244e-01, -4.5748e-01,  8.3363e-02,  1.6080e-01,\n",
       "          1.3813e-03,  4.1321e-01,  8.3819e-02, -2.5578e-02,  2.6782e-02,\n",
       "          2.7782e-01, -1.7845e-01,  1.4933e-01, -1.4904e-01,  1.5547e-01,\n",
       "         -2.9961e-01, -9.7159e-02,  1.1874e-02, -1.9497e-02,  2.4088e-01,\n",
       "          2.4776e-01,  1.9448e-02, -4.0194e-02,  4.5451e-01, -1.6213e-01,\n",
       "         -9.0387e-03,  3.2908e-01, -1.5880e-01,  3.9583e-02,  2.5313e-01,\n",
       "          3.5398e-02, -4.8262e-02, -3.0094e-02, -1.1079e-01, -3.9099e-02,\n",
       "         -4.5695e-01,  8.4270e-02, -4.4468e-02, -6.9188e-02,  1.8748e-01,\n",
       "         -6.7908e-02,  3.3535e-03,  7.8092e-03, -4.8371e-01, -1.2673e-01,\n",
       "         -2.5799e-02, -2.8293e-01, -7.6424e-02,  5.0793e-01, -2.2678e-01,\n",
       "         -2.1026e-01, -9.4730e-02,  3.1274e-01,  1.4297e-01,  2.1836e-02,\n",
       "          8.5986e-02,  2.7609e-01,  2.7106e-02, -3.9507e-01,  2.7772e-01,\n",
       "          1.6772e-01,  2.0392e-01,  2.4315e-02,  2.4308e-01, -4.3086e-01,\n",
       "          3.1683e-01, -3.0984e-01, -1.1825e-01, -9.7634e-02,  8.0094e-02,\n",
       "         -1.4221e-01,  2.9731e-01, -7.9446e-02, -4.8730e-01,  1.5322e-02,\n",
       "         -2.1076e-01,  4.5793e-02, -3.6050e-02, -4.7809e-02, -2.4399e-01,\n",
       "         -1.6253e-02,  8.2101e-02,  7.8659e-03, -2.6124e-01,  4.1198e-02,\n",
       "          5.1005e-02, -2.9525e-03, -3.3699e-01,  2.6410e-01,  1.0203e-01,\n",
       "         -4.3313e-01,  4.1545e-01, -1.8098e-02,  3.9197e-01, -3.4087e-01,\n",
       "         -1.3986e-01,  1.6644e-01,  1.5676e-01, -2.7380e-01,  4.8115e-02,\n",
       "         -3.1412e-01, -3.3218e-02, -2.6742e-02, -3.6407e-01, -6.9017e-02,\n",
       "          1.5290e-01,  1.4284e-01,  6.3094e-02, -1.7220e-01, -7.9393e-03,\n",
       "         -2.3329e-01, -2.3385e-01, -1.6247e-01,  2.2047e-01,  1.3658e-01,\n",
       "         -1.7160e-01,  3.4056e-01,  1.8547e-01, -3.6659e-01,  1.2483e-01,\n",
       "          2.3268e-01, -3.6866e-01,  2.0118e-01,  1.0062e-01, -1.1363e-01,\n",
       "         -3.5851e-01, -1.4581e-01, -2.1665e-01,  2.8467e-01,  2.1944e-01,\n",
       "         -8.6968e-02,  3.3603e-01, -1.3291e-01,  7.6470e-02, -3.9384e-01,\n",
       "          8.7070e-02,  1.8656e-01,  3.3074e-01,  8.4197e-02, -2.3073e-01,\n",
       "         -5.0470e-02,  3.0444e-01,  3.2446e-01,  1.9457e-03, -1.4427e-01,\n",
       "         -6.4884e-02, -5.6954e-02,  7.5328e-02,  6.0543e-03, -1.6515e-01,\n",
       "         -2.0974e-01,  1.7649e-01,  2.5276e-01,  2.7464e-02, -3.4663e-02,\n",
       "          1.1239e-01,  3.5827e-01, -2.8111e-01,  3.5358e-02,  4.9653e-02,\n",
       "         -1.1553e-01,  1.3479e-02,  1.1016e-01,  2.6401e-01, -3.4379e-02,\n",
       "          2.1285e-01,  2.1025e-01, -1.6921e-01, -6.7291e-02, -9.5234e-02,\n",
       "         -3.6467e-01,  1.2736e-01, -4.3015e-01,  3.8770e-02,  6.3262e-02,\n",
       "          4.0906e-03,  1.2522e-01, -4.5083e-02, -6.1746e-03,  1.0449e-01,\n",
       "         -2.8166e-01, -6.7917e-02,  1.9363e-01, -2.5945e-01,  9.3389e-02,\n",
       "         -1.4383e-01, -3.6516e-01,  9.8813e-02, -9.4105e-02,  4.1844e-02,\n",
       "          2.4313e-01, -5.8586e-02, -5.6884e-02, -2.4146e-01, -2.7238e-02,\n",
       "         -2.9969e-01, -3.0211e-01,  3.1738e-01,  8.1288e-02, -5.4190e-02,\n",
       "         -1.2376e-01, -4.9828e-02, -4.8127e-02, -1.5888e-01, -2.4704e-01,\n",
       "         -1.2685e-01, -5.7327e-02,  5.0149e-02, -1.3170e-01, -7.1959e-02,\n",
       "          6.1776e-02, -1.8514e-01,  2.4813e-02, -1.1305e-01,  8.9890e-02,\n",
       "          8.5525e-02, -2.1873e-01, -1.8055e-01, -2.9958e-01, -1.7488e-01,\n",
       "          2.5030e-01, -4.1080e-02, -2.3178e-02,  1.0520e-01,  2.1074e-01,\n",
       "          3.1270e-01, -1.4219e-01, -2.8470e-02, -7.1510e-02,  7.3036e-02,\n",
       "         -1.8335e-01,  4.7342e-02, -6.3725e-02, -5.0386e-02,  1.8333e-01,\n",
       "          1.4460e-01,  1.8819e-01, -3.4915e-03,  1.8639e-01,  4.0673e-01,\n",
       "         -3.3623e-01, -7.8663e-04, -1.2423e-01, -2.4671e-01,  1.2208e-01,\n",
       "         -3.9579e-01, -3.6336e-02, -8.8275e-02,  6.9799e-02,  1.1515e-01,\n",
       "          3.9912e-02,  1.5816e-01,  4.7877e-01, -2.9517e-01, -1.9245e-01,\n",
       "          2.3582e-01,  5.0040e-01,  1.2628e-01, -3.6173e-01, -1.0846e-02,\n",
       "          2.1432e-01,  1.2097e-01, -1.4738e-01,  1.8796e-01,  5.3722e-02,\n",
       "          2.7100e-01,  1.3108e-01, -3.0986e-03, -9.2006e-02,  3.4008e-01,\n",
       "          9.6987e-02,  2.9751e-02,  7.8753e-02,  2.5953e-01,  1.8564e-01,\n",
       "         -3.4745e-01, -1.1513e-01, -2.7325e-01,  1.8787e-01, -5.9595e-02,\n",
       "          4.9905e-02, -4.6738e-02, -7.1610e-02, -2.3592e-01,  5.0210e-02,\n",
       "          4.2938e-01, -1.0434e-01,  5.5898e-02, -6.1959e-02, -7.7933e-02,\n",
       "          1.0612e-01, -2.9068e-01, -1.6760e-01,  1.5046e-01, -3.1198e-02,\n",
       "         -1.4458e-01, -1.9105e-01,  1.3221e-01, -1.3021e-01, -5.3511e-02,\n",
       "          3.0757e-01,  2.7805e-01,  1.1379e-01,  3.0156e-02, -6.3840e-03,\n",
       "          3.7170e-01,  2.0467e-01, -5.0029e-02, -2.5668e-01, -4.8801e-02,\n",
       "          3.4724e-02, -1.9134e-01,  3.0109e-01, -4.4690e-02,  2.1682e-02,\n",
       "          2.1775e-01,  1.4079e-02,  7.7059e-03,  1.6973e-01,  2.0835e-01,\n",
       "          1.4216e-01, -5.7731e-02, -1.1384e-01, -3.6062e-01, -1.3935e-01,\n",
       "         -2.2808e-01, -1.0972e-01,  2.0820e-02,  1.4127e-01,  3.4593e-01,\n",
       "         -9.4599e-02,  3.2318e-01, -3.4793e-01, -2.2841e-01, -1.3870e-01,\n",
       "         -2.2551e-01,  2.0208e-01,  2.1096e-01,  1.9063e-01,  2.3811e-01,\n",
       "          1.0764e-01, -3.6371e-02, -2.6057e-01,  2.2809e-02, -1.5918e-02,\n",
       "          1.6952e-01,  1.4532e-01, -1.1566e-02,  2.9290e-02,  1.3691e-01,\n",
       "          3.6294e-03, -3.1097e-01,  3.8894e-02,  8.9511e-02, -2.4559e-01,\n",
       "          3.4412e-01,  2.8584e-01, -7.4922e-02, -9.6098e-02, -2.7859e-01,\n",
       "         -1.1786e-01, -1.3959e-02, -1.9478e-01, -6.0740e-02, -2.5595e-01,\n",
       "          3.5289e-02,  1.0516e-01,  1.1509e-02,  4.2218e-01,  5.3241e-01,\n",
       "          8.2289e-02, -2.1476e-01, -1.8364e-01, -2.8287e-01, -4.7442e-02,\n",
       "          3.6418e-02, -1.0004e-01,  1.4397e-01,  5.4881e-01,  1.1615e-01,\n",
       "          1.0881e-01, -1.3113e-02,  4.4165e-01, -2.1154e-01,  1.9887e-01,\n",
       "         -2.3201e-01, -3.8439e-02,  1.3247e-01,  4.5898e-02,  3.5681e-03,\n",
       "          2.8840e-01, -3.4101e-01, -1.3250e-01,  3.1379e-02,  1.8129e-01,\n",
       "          1.6333e-01, -2.3487e-02,  5.1262e-01,  1.2675e-01, -5.0250e-02,\n",
       "          3.2816e-02, -3.7918e-02,  8.9821e-02, -8.8036e-02,  3.6810e-01,\n",
       "         -6.3277e-02, -3.9600e-01,  2.9552e-01, -7.4217e-02, -3.2124e-01,\n",
       "          1.1370e-03,  2.9951e-01,  1.2760e-03,  6.6064e-02,  2.6129e-01,\n",
       "          2.1038e-01,  5.6630e-02, -9.0695e-02, -3.4116e-01, -1.3164e-01,\n",
       "          1.3704e-02,  3.8093e-02,  4.1691e-01, -1.5666e-01,  2.7354e-01,\n",
       "          8.7664e-03,  4.8012e-02,  3.7859e-01,  7.8286e-02, -8.0737e-02,\n",
       "         -2.6279e-01,  2.2989e-01,  5.9378e-02, -2.0865e-01,  1.8409e-01,\n",
       "         -2.4377e-02,  2.2591e-01,  5.4951e-02,  3.4647e-01, -6.4766e-02,\n",
       "          1.5379e-02,  4.2052e-01, -1.0564e-01,  1.7925e-01,  1.6929e-01,\n",
       "          1.1215e-01, -1.7638e-01,  2.6777e-02,  3.3063e-01,  1.5712e-02,\n",
       "         -1.2517e-01,  1.7321e-01,  3.9652e-01,  2.3057e-01,  1.1005e-01,\n",
       "         -2.1206e-01,  1.3877e-01,  3.1534e-01,  9.1146e-02, -2.8163e-01,\n",
       "         -5.9605e-02,  1.9173e-01,  7.0849e-02,  3.9846e-02,  4.6931e-02,\n",
       "         -3.5598e-01,  1.3598e-01,  8.9619e-02, -9.8157e-02, -1.0389e-01,\n",
       "         -2.3553e-01,  8.7716e-02, -1.0404e-01, -3.9487e-01, -2.1300e-01,\n",
       "          2.5726e-02,  1.6104e-02, -5.8521e-02, -3.6555e-01, -1.7610e-01,\n",
       "         -2.5911e-01, -9.3786e-03,  4.1170e-02,  1.5552e-02,  2.3371e-01,\n",
       "          3.8034e-01,  1.1459e-01,  2.2010e-01, -2.9182e-02, -2.1185e-01,\n",
       "         -1.0078e-01,  1.6090e-01, -2.5305e-01,  9.0957e-02, -1.0630e-01,\n",
       "          1.3166e-01,  2.8412e-02, -1.1403e-01, -2.3070e-01,  1.9142e-01,\n",
       "          7.2827e-02,  1.0077e-01, -8.5275e-02, -7.6405e-02,  2.8028e-02,\n",
       "          1.0881e-01,  3.6664e-01, -2.1130e-01,  4.2561e-02, -1.2023e-01,\n",
       "         -1.0817e-01,  1.5248e-02,  1.4776e-01,  1.7676e-01,  1.5140e-01,\n",
       "          2.6131e-01, -1.0666e-01,  1.0539e-02,  2.1011e-01,  9.4795e-02,\n",
       "         -3.2175e-02,  1.2351e-01, -1.8147e-02,  2.9107e-02, -4.8487e-02,\n",
       "          2.2995e-01, -3.6445e-02, -2.5212e-01,  2.5256e-01, -7.6358e-03,\n",
       "          3.2285e-01, -2.7538e-01,  6.1627e-03, -1.9838e-01,  1.0047e-01,\n",
       "          9.3331e-02, -1.7859e-01, -1.2384e-01,  3.1713e-01, -2.1902e-01,\n",
       "         -1.1613e-01,  3.1179e-01, -2.4689e-02, -3.3935e-01, -1.8425e-01,\n",
       "         -8.8343e-02,  1.7959e-01, -4.2800e-03,  9.3990e-02, -8.2420e-02,\n",
       "          3.5891e-01, -9.6190e-02, -2.3118e-01, -3.3623e-01,  9.6662e-02,\n",
       "         -1.2626e-01, -1.9176e-01,  2.6696e-01,  2.7641e-01, -1.5231e-01,\n",
       "          1.8127e-01,  3.2665e-02,  2.4520e-01,  2.3004e-01,  7.9890e-02,\n",
       "          3.4158e-02, -5.2496e-02, -5.0806e-03, -2.6299e-01, -1.0342e-01,\n",
       "         -2.0025e-01, -1.9415e-01, -6.6728e-02, -2.6080e-01, -5.4599e-01,\n",
       "          1.1557e-01,  2.1583e-01, -3.4713e-01, -1.5882e-01, -5.9560e-03,\n",
       "          3.8051e-01,  3.2214e-01, -3.5320e-01, -8.4185e-02, -1.1656e-01,\n",
       "         -1.0125e-01, -2.8393e-01,  3.2832e-01, -1.4416e-01,  1.0282e-01,\n",
       "         -5.7437e-02, -1.3223e-02, -2.4799e-02,  6.1816e-04,  7.0731e-02,\n",
       "         -1.0808e-01, -1.4009e-01,  1.7373e-01, -1.4521e-01, -7.8702e-02,\n",
       "         -6.0216e-02, -4.2313e-02, -2.9258e-01,  8.2496e-02, -1.2742e-01,\n",
       "          3.3914e-01,  7.5809e-02, -4.8059e-02, -2.9438e-01,  2.7892e-01,\n",
       "          1.4789e-01,  7.2331e-02, -7.9040e-02,  1.3006e-01,  2.0975e-01,\n",
       "          1.7561e-02, -3.4268e-01, -2.7370e-01, -1.9998e-01,  2.5381e-01,\n",
       "          1.8589e-01, -3.4034e-01,  2.0634e-01,  1.9083e-02,  2.3755e-01,\n",
       "          1.3653e-01,  2.3858e-01,  6.6519e-02, -7.3879e-02, -2.4172e-01,\n",
       "         -4.0096e-02, -7.3598e-02, -2.2265e-01, -8.8047e-02, -1.3823e-01,\n",
       "         -5.0333e-02, -2.6405e-01, -3.0174e-01, -4.2550e-01, -6.0004e-02,\n",
       "          3.2633e-01,  7.6712e-03, -3.5559e-01,  1.8749e-01, -6.5799e-02,\n",
       "         -1.2707e-02,  3.7685e-01, -2.6955e-01, -3.2532e-02, -2.5805e-01,\n",
       "         -3.1422e-02, -1.4983e-01,  2.7979e-01,  1.6932e-02, -2.4224e-01,\n",
       "          2.4903e-01, -4.3664e-02, -1.3503e-01,  6.7023e-02,  1.0661e-01,\n",
       "         -2.9826e-01, -4.2024e-01, -1.3812e-01, -1.9036e-01, -1.3940e-02,\n",
       "          5.1796e-01,  9.8783e-02,  3.1564e-01,  1.5257e-01,  2.2063e-02,\n",
       "          1.7211e-01,  2.7012e-01, -1.4930e-01,  1.6983e-01,  8.8252e-02,\n",
       "          9.5880e-02, -1.6698e-01, -7.2178e-02,  2.6378e-01,  9.5576e-02,\n",
       "          1.3019e-01,  1.0060e-02,  5.7025e-02,  1.6329e-01, -2.1116e-01,\n",
       "          1.4585e-01,  2.3094e-01,  4.8238e-01,  1.4406e-01, -1.5869e-01,\n",
       "         -1.0522e-01,  6.8848e-02,  8.0928e-03,  1.8317e-01,  2.9984e-01,\n",
       "         -1.2757e-01, -5.1485e-01, -2.9127e-01, -4.7411e-02,  1.0825e-01,\n",
       "         -4.5399e-02, -4.4384e-01,  3.5277e-01, -2.5786e-01,  2.8086e-01,\n",
       "         -5.8258e-02,  2.4465e-01, -6.2713e-02]]), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='vinai/phobert-base', vocab_size=64000, model_max_len=256, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(64001, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(258, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phobert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvi import ViTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chúng_tôi là những nghiên_cứu_viên .'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ViTokenizer.tokenize(u\"Chúng tôi là những nghiên cứu viên.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
