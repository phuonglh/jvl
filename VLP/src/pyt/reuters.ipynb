{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.datasets import AG_NEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = AG_NEWS(root='../../dat/pyt', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iter = AG_NEWS(root='../../dat/pyt', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " 'Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\\\which has a reputation for making well-timed and occasionally\\\\controversial plays in the defense industry, has quietly placed\\\\its bets on another part of the market.')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " \"Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\\\about the economy and the outlook for earnings are expected to\\\\hang over the stock market next week during the depth of the\\\\summer doldrums.\")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter):\n",
    "  for _, text in data_iter:\n",
    "    yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95808"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[475, 21, 30, 5297]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(['here', 'is', 'an', 'example'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[475, 21, 30, 5297]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pipeline('here is an example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_pipeline('9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "  ys, xs, offsets = [], [], [0]\n",
    "  for (y, x) in batch:\n",
    "    ys.append(label_pipeline(y))\n",
    "    ts = torch.tensor(text_pipeline(x), dtype=torch.int64)\n",
    "    xs.append(ts)\n",
    "    offsets.append(ts.size(0))\n",
    "  ys = torch.tensor(ys, dtype=torch.int64)\n",
    "  offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "  xs = torch.cat(xs)\n",
    "  return ys.to(device), xs.to(device), offsets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = AG_NEWS(root='../../dat/pyt', split='train')\n",
    "training_data_loader = DataLoader(train_iter, batch_size=32, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3750"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "  def __init__(self, vocab_size, embed_dim, num_class):\n",
    "    super(NeuralNetwork, self).__init__()\n",
    "    self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "    self.linear = nn.Linear(embed_dim, num_class)\n",
    "    self.init_weights()\n",
    "\n",
    "  def init_weights(self):\n",
    "    self.embedding.weight.data.uniform_(-0.5, 0.5)\n",
    "    self.linear.weight.data.uniform_(-0.5, 0.5)\n",
    "    self.linear.bias.data.zero_()\n",
    "  \n",
    "  def forward(self, text, offsets):\n",
    "    embeded = self.embedding(text, offsets) # compute mean vectors of all words in the text\n",
    "    return self.linear(embeded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = len(set([label for (label, _) in train_iter]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# 1 : World\n",
    "# 2 : Sports\n",
    "# 3 : Business\n",
    "# 4 : Sci/Tec\n",
    "print(num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter = AG_NEWS(root='../../dat/pyt', split='train')\n",
    "next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95808\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(vocab_size, 64, num_class).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (embedding): EmbeddingBag(95808, 64, mode=mean)\n",
       "  (linear): Linear(in_features=64, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() # useful when the output is an unnormalized score vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, criterion, optimizer):\n",
    "  N = len(dataloader.dataset)\n",
    "  model.train()\n",
    "  start_time = time.time()\n",
    "  for batch, (ys, xs, offsets) in enumerate(dataloader):\n",
    "    zs = model(xs, offsets)\n",
    "    loss = criterion(zs, ys)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if batch % 500 == 0:\n",
    "      elapsed = time.time() - start_time\n",
    "      loss, current = loss.item(), batch * len(xs)\n",
    "      print(f\"loss: {loss:>7f} [{current:>5d}/{N:>5d}], elapsed: {elapsed}\")\n",
    "      start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, criterion):\n",
    "  N = len(dataloader.dataset)\n",
    "  num_batch = len(dataloader)\n",
    "  model.eval()\n",
    "  test_loss, correct = 0, 0\n",
    "  with torch.no_grad():\n",
    "    for (ys, xs, offsets) in dataloader:\n",
    "      zs = model(xs, offsets)\n",
    "      test_loss += criterion(zs, ys).item()\n",
    "      correct += (zs.argmax(1) == ys).type(torch.float).sum().item()\n",
    "  test_loss /= num_batch\n",
    "  correct /= N\n",
    "  print(f\"Test accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------\n",
      "loss: 1.426393 [    0/120000], elapsed: 0.007916688919067383\n",
      "loss: 1.370969 [641000/120000], elapsed: 2.0563488006591797\n",
      "loss: 1.347753 [1315000/120000], elapsed: 1.908263921737671\n",
      "loss: 1.324668 [2068500/120000], elapsed: 1.9025630950927734\n",
      "loss: 1.374274 [2690000/120000], elapsed: 1.9186592102050781\n",
      "loss: 1.381827 [3127500/120000], elapsed: 1.8854007720947266\n",
      "loss: 1.351782 [4032000/120000], elapsed: 1.878354787826538\n",
      "loss: 1.333173 [4788000/120000], elapsed: 1.878736972808838\n",
      "Test accuracy: 30.2%, Avg loss: 1.371654 \n",
      "\n",
      "Epoch 2\n",
      "-------\n",
      "loss: 1.388052 [    0/120000], elapsed: 0.004431247711181641\n",
      "loss: 1.359332 [641000/120000], elapsed: 1.9160959720611572\n",
      "loss: 1.324045 [1315000/120000], elapsed: 1.9002797603607178\n",
      "loss: 1.305135 [2068500/120000], elapsed: 1.9554660320281982\n",
      "loss: 1.360818 [2690000/120000], elapsed: 1.9280998706817627\n",
      "loss: 1.357424 [3127500/120000], elapsed: 1.9295012950897217\n",
      "loss: 1.333590 [4032000/120000], elapsed: 1.9556457996368408\n",
      "loss: 1.316975 [4788000/120000], elapsed: 2.094158887863159\n",
      "Test accuracy: 35.8%, Avg loss: 1.350643 \n",
      "\n",
      "Epoch 3\n",
      "-------\n",
      "loss: 1.377082 [    0/120000], elapsed: 0.0042018890380859375\n",
      "loss: 1.347951 [641000/120000], elapsed: 1.9800260066986084\n",
      "loss: 1.298411 [1315000/120000], elapsed: 2.1979153156280518\n",
      "loss: 1.283780 [2068500/120000], elapsed: 2.7726681232452393\n",
      "loss: 1.346445 [2690000/120000], elapsed: 2.172560691833496\n",
      "loss: 1.332606 [3127500/120000], elapsed: 1.9328057765960693\n",
      "loss: 1.314127 [4032000/120000], elapsed: 1.9976749420166016\n",
      "loss: 1.299467 [4788000/120000], elapsed: 1.8873116970062256\n",
      "Test accuracy: 41.6%, Avg loss: 1.328079 \n",
      "\n",
      "Epoch 4\n",
      "-------\n",
      "loss: 1.367056 [    0/120000], elapsed: 0.004673004150390625\n",
      "loss: 1.335117 [641000/120000], elapsed: 2.2172250747680664\n",
      "loss: 1.269655 [1315000/120000], elapsed: 1.903799057006836\n",
      "loss: 1.259445 [2068500/120000], elapsed: 2.1946310997009277\n",
      "loss: 1.330317 [2690000/120000], elapsed: 2.535818099975586\n",
      "loss: 1.305147 [3127500/120000], elapsed: 2.301002025604248\n",
      "loss: 1.292148 [4032000/120000], elapsed: 1.8666858673095703\n",
      "loss: 1.279910 [4788000/120000], elapsed: 1.867750883102417\n",
      "Test accuracy: 46.2%, Avg loss: 1.302623 \n",
      "\n",
      "Epoch 5\n",
      "-------\n",
      "loss: 1.356868 [    0/120000], elapsed: 0.004274845123291016\n",
      "loss: 1.320418 [641000/120000], elapsed: 2.158649206161499\n",
      "loss: 1.236464 [1315000/120000], elapsed: 2.0044198036193848\n",
      "loss: 1.231336 [2068500/120000], elapsed: 2.0730719566345215\n",
      "loss: 1.312063 [2690000/120000], elapsed: 1.8964958190917969\n",
      "loss: 1.273782 [3127500/120000], elapsed: 1.874488115310669\n",
      "loss: 1.266993 [4032000/120000], elapsed: 1.863821029663086\n",
      "loss: 1.258026 [4788000/120000], elapsed: 1.9333720207214355\n",
      "Test accuracy: 49.2%, Avg loss: 1.273608 \n",
      "\n",
      "Epoch 6\n",
      "-------\n",
      "loss: 1.345491 [    0/120000], elapsed: 0.0041959285736083984\n",
      "loss: 1.303742 [641000/120000], elapsed: 1.9067230224609375\n",
      "loss: 1.198161 [1315000/120000], elapsed: 1.8823039531707764\n",
      "loss: 1.199063 [2068500/120000], elapsed: 2.048468828201294\n",
      "loss: 1.291701 [2690000/120000], elapsed: 2.0866191387176514\n",
      "loss: 1.238186 [3127500/120000], elapsed: 1.9235780239105225\n",
      "loss: 1.238549 [4032000/120000], elapsed: 1.8478341102600098\n",
      "loss: 1.233886 [4788000/120000], elapsed: 1.8552687168121338\n",
      "Test accuracy: 51.7%, Avg loss: 1.241018 \n",
      "\n",
      "Epoch 7\n",
      "-------\n",
      "loss: 1.331916 [    0/120000], elapsed: 0.004591941833496094\n",
      "loss: 1.285150 [641000/120000], elapsed: 1.912065029144287\n",
      "loss: 1.154710 [1315000/120000], elapsed: 1.905534267425537\n",
      "loss: 1.162599 [2068500/120000], elapsed: 1.877540111541748\n",
      "loss: 1.269484 [2690000/120000], elapsed: 2.0892350673675537\n",
      "loss: 1.198849 [3127500/120000], elapsed: 2.0284531116485596\n",
      "loss: 1.207126 [4032000/120000], elapsed: 1.874300241470337\n",
      "loss: 1.207703 [4788000/120000], elapsed: 2.029636859893799\n",
      "Test accuracy: 54.1%, Avg loss: 1.205334 \n",
      "\n",
      "Epoch 8\n",
      "-------\n",
      "loss: 1.315174 [    0/120000], elapsed: 0.004356861114501953\n",
      "loss: 1.264679 [641000/120000], elapsed: 1.9235491752624512\n",
      "loss: 1.106609 [1315000/120000], elapsed: 1.8793199062347412\n",
      "loss: 1.122197 [2068500/120000], elapsed: 1.9318971633911133\n",
      "loss: 1.245654 [2690000/120000], elapsed: 1.8845670223236084\n",
      "loss: 1.156745 [3127500/120000], elapsed: 1.8892250061035156\n",
      "loss: 1.173253 [4032000/120000], elapsed: 1.9992611408233643\n",
      "loss: 1.179633 [4788000/120000], elapsed: 1.8477869033813477\n",
      "Test accuracy: 56.4%, Avg loss: 1.167278 \n",
      "\n",
      "Epoch 9\n",
      "-------\n",
      "loss: 1.294449 [    0/120000], elapsed: 0.004164218902587891\n",
      "loss: 1.242183 [641000/120000], elapsed: 1.8823087215423584\n",
      "loss: 1.054678 [1315000/120000], elapsed: 1.887256145477295\n",
      "loss: 1.078329 [2068500/120000], elapsed: 1.8748540878295898\n",
      "loss: 1.220254 [2690000/120000], elapsed: 1.8527309894561768\n",
      "loss: 1.112924 [3127500/120000], elapsed: 2.0212271213531494\n",
      "loss: 1.137510 [4032000/120000], elapsed: 1.8674349784851074\n",
      "loss: 1.149717 [4788000/120000], elapsed: 1.849726915359497\n",
      "Test accuracy: 58.7%, Avg loss: 1.127574 \n",
      "\n",
      "Epoch 10\n",
      "-------\n",
      "loss: 1.269255 [    0/120000], elapsed: 0.004205226898193359\n",
      "loss: 1.217401 [641000/120000], elapsed: 1.925907850265503\n",
      "loss: 0.999858 [1315000/120000], elapsed: 1.8817708492279053\n",
      "loss: 1.031716 [2068500/120000], elapsed: 1.8409090042114258\n",
      "loss: 1.193137 [2690000/120000], elapsed: 1.8486957550048828\n",
      "loss: 1.068276 [3127500/120000], elapsed: 1.9582359790802002\n",
      "loss: 1.100503 [4032000/120000], elapsed: 1.8472700119018555\n",
      "loss: 1.118015 [4788000/120000], elapsed: 1.8250820636749268\n",
      "Test accuracy: 60.6%, Avg loss: 1.086867 \n",
      "\n",
      "Epoch 11\n",
      "-------\n",
      "loss: 1.239591 [    0/120000], elapsed: 0.00411677360534668\n",
      "loss: 1.190191 [641000/120000], elapsed: 1.8648550510406494\n",
      "loss: 0.943134 [1315000/120000], elapsed: 1.8907928466796875\n",
      "loss: 0.983371 [2068500/120000], elapsed: 1.894216775894165\n",
      "loss: 1.164130 [2690000/120000], elapsed: 1.8845293521881104\n",
      "loss: 1.023509 [3127500/120000], elapsed: 1.8958427906036377\n",
      "loss: 1.062907 [4032000/120000], elapsed: 1.8455190658569336\n",
      "loss: 1.084764 [4788000/120000], elapsed: 1.9032011032104492\n",
      "Test accuracy: 62.8%, Avg loss: 1.045748 \n",
      "\n",
      "Epoch 12\n",
      "-------\n",
      "loss: 1.205988 [    0/120000], elapsed: 0.004288673400878906\n",
      "loss: 1.160768 [641000/120000], elapsed: 1.8839170932769775\n",
      "loss: 0.885540 [1315000/120000], elapsed: 1.9171452522277832\n",
      "loss: 0.934568 [2068500/120000], elapsed: 1.8375329971313477\n",
      "loss: 1.133241 [2690000/120000], elapsed: 1.847261905670166\n",
      "loss: 0.979225 [3127500/120000], elapsed: 1.8737499713897705\n",
      "loss: 1.025481 [4032000/120000], elapsed: 1.954131841659546\n",
      "loss: 1.050462 [4788000/120000], elapsed: 2.0151119232177734\n",
      "Test accuracy: 64.8%, Avg loss: 1.004811 \n",
      "\n",
      "Epoch 13\n",
      "-------\n",
      "loss: 1.169426 [    0/120000], elapsed: 0.004123210906982422\n",
      "loss: 1.129772 [641000/120000], elapsed: 1.9358880519866943\n",
      "loss: 0.828175 [1315000/120000], elapsed: 1.9270761013031006\n",
      "loss: 0.886702 [2068500/120000], elapsed: 1.8745551109313965\n",
      "loss: 1.100766 [2690000/120000], elapsed: 2.027966022491455\n",
      "loss: 0.935980 [3127500/120000], elapsed: 1.837475061416626\n",
      "loss: 0.989002 [4032000/120000], elapsed: 1.8442270755767822\n",
      "loss: 1.015808 [4788000/120000], elapsed: 1.8899681568145752\n",
      "Test accuracy: 66.6%, Avg loss: 0.964659 \n",
      "\n",
      "Epoch 14\n",
      "-------\n",
      "loss: 1.131150 [    0/120000], elapsed: 0.00537109375\n",
      "loss: 1.098159 [641000/120000], elapsed: 2.245790958404541\n",
      "loss: 0.772152 [1315000/120000], elapsed: 2.472085952758789\n",
      "loss: 0.841080 [2068500/120000], elapsed: 1.9770519733428955\n",
      "loss: 1.067288 [2690000/120000], elapsed: 1.9231271743774414\n",
      "loss: 0.894282 [3127500/120000], elapsed: 1.949718713760376\n",
      "loss: 0.954158 [4032000/120000], elapsed: 2.0412909984588623\n",
      "loss: 0.981562 [4788000/120000], elapsed: 1.9007980823516846\n",
      "Test accuracy: 68.3%, Avg loss: 0.925856 \n",
      "\n",
      "Epoch 15\n",
      "-------\n",
      "loss: 1.092444 [    0/120000], elapsed: 0.004317283630371094\n",
      "loss: 1.066981 [641000/120000], elapsed: 1.8818449974060059\n",
      "loss: 0.718489 [1315000/120000], elapsed: 1.891097068786621\n",
      "loss: 0.798712 [2068500/120000], elapsed: 2.025177001953125\n",
      "loss: 1.033566 [2690000/120000], elapsed: 1.9700541496276855\n",
      "loss: 0.854542 [3127500/120000], elapsed: 1.843501091003418\n",
      "loss: 0.921441 [4032000/120000], elapsed: 1.9836640357971191\n",
      "loss: 0.948376 [4788000/120000], elapsed: 1.9320640563964844\n",
      "Test accuracy: 69.6%, Avg loss: 0.888872 \n",
      "\n",
      "Epoch 16\n",
      "-------\n",
      "loss: 1.054423 [    0/120000], elapsed: 0.00476527214050293\n",
      "loss: 1.037162 [641000/120000], elapsed: 1.923825740814209\n",
      "loss: 0.667991 [1315000/120000], elapsed: 1.9168660640716553\n",
      "loss: 0.760196 [2068500/120000], elapsed: 1.877312183380127\n",
      "loss: 1.000383 [2690000/120000], elapsed: 1.8852131366729736\n",
      "loss: 0.817036 [3127500/120000], elapsed: 1.8571629524230957\n",
      "loss: 0.891110 [4032000/120000], elapsed: 1.8632941246032715\n",
      "loss: 0.916687 [4788000/120000], elapsed: 1.8577001094818115\n",
      "Test accuracy: 71.2%, Avg loss: 0.854037 \n",
      "\n",
      "Epoch 17\n",
      "-------\n",
      "loss: 1.017915 [    0/120000], elapsed: 0.00403594970703125\n",
      "loss: 1.009365 [641000/120000], elapsed: 1.8738911151885986\n",
      "loss: 0.621191 [1315000/120000], elapsed: 1.8512330055236816\n",
      "loss: 0.725723 [2068500/120000], elapsed: 1.856381893157959\n",
      "loss: 0.968400 [2690000/120000], elapsed: 1.878067970275879\n",
      "loss: 0.781898 [3127500/120000], elapsed: 1.8262009620666504\n",
      "loss: 0.863208 [4032000/120000], elapsed: 1.8564441204071045\n",
      "loss: 0.886688 [4788000/120000], elapsed: 1.8573122024536133\n",
      "Test accuracy: 72.6%, Avg loss: 0.821534 \n",
      "\n",
      "Epoch 18\n",
      "-------\n",
      "loss: 0.983424 [    0/120000], elapsed: 0.003988981246948242\n",
      "loss: 0.983943 [641000/120000], elapsed: 1.880500078201294\n",
      "loss: 0.578349 [1315000/120000], elapsed: 1.8660149574279785\n",
      "loss: 0.695168 [2068500/120000], elapsed: 1.8644750118255615\n",
      "loss: 0.938081 [2690000/120000], elapsed: 1.8699278831481934\n",
      "loss: 0.749135 [3127500/120000], elapsed: 1.9072718620300293\n",
      "loss: 0.837623 [4032000/120000], elapsed: 1.856827974319458\n",
      "loss: 0.858369 [4788000/120000], elapsed: 1.8495769500732422\n",
      "Test accuracy: 73.8%, Avg loss: 0.791417 \n",
      "\n",
      "Epoch 19\n",
      "-------\n",
      "loss: 0.951178 [    0/120000], elapsed: 0.0042150020599365234\n",
      "loss: 0.960988 [641000/120000], elapsed: 1.8695201873779297\n",
      "loss: 0.539502 [1315000/120000], elapsed: 1.883587121963501\n",
      "loss: 0.668210 [2068500/120000], elapsed: 1.8721470832824707\n",
      "loss: 0.909682 [2690000/120000], elapsed: 1.8498880863189697\n",
      "loss: 0.718664 [3127500/120000], elapsed: 1.839066982269287\n",
      "loss: 0.814157 [4032000/120000], elapsed: 1.851593017578125\n",
      "loss: 0.831599 [4788000/120000], elapsed: 1.8447790145874023\n",
      "Test accuracy: 74.7%, Avg loss: 0.763645 \n",
      "\n",
      "Epoch 20\n",
      "-------\n",
      "loss: 0.921205 [    0/120000], elapsed: 0.004220247268676758\n",
      "loss: 0.940399 [641000/120000], elapsed: 1.9233200550079346\n",
      "loss: 0.504525 [1315000/120000], elapsed: 1.9303219318389893\n",
      "loss: 0.644450 [2068500/120000], elapsed: 1.945363998413086\n",
      "loss: 0.883286 [2690000/120000], elapsed: 1.957794189453125\n",
      "loss: 0.690347 [3127500/120000], elapsed: 2.0637853145599365\n",
      "loss: 0.792576 [4032000/120000], elapsed: 1.9948790073394775\n",
      "loss: 0.806199 [4788000/120000], elapsed: 1.9259858131408691\n",
      "Test accuracy: 75.6%, Avg loss: 0.738114 \n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "epochs = 20 # 5 minutes on my MBP\n",
    "for t in range(epochs):\n",
    "  train_iter = AG_NEWS(root='../../dat/pyt', split='train')\n",
    "  test_iter = AG_NEWS(root='../../dat/pyt', split='test')\n",
    "  training_data_loader = DataLoader(train_iter, batch_size=32, shuffle=False, collate_fn=collate_batch)\n",
    "  test_data_loader = DataLoader(test_iter, batch_size=32, shuffle=False, collate_fn=collate_batch)  \n",
    "  print(f\"Epoch {t+1}\\n-------\")\n",
    "  train(training_data_loader, model, criterion, optimizer)\n",
    "  test(test_data_loader, model, criterion)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
